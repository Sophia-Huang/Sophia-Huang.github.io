<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2"/>

<link rel="stylesheet" href="/css/main.css?v=6.7.0"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Lecture 1: Course Introduction [slice]1.1 Computer vision overview Related Courses @ Stanford  CS131: Undergraduate introductory class cs224n: intersection of deep learning and natural language proces">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Convolutional Neural Networks for Visual Recognition|李飞飞">
<meta property="og:url" content="http://yoursite.com/2018/09/28/video-cs231n-李飞飞/index.html">
<meta property="og:site_name" content="玉琳的博客">
<meta property="og:description" content="Lecture 1: Course Introduction [slice]1.1 Computer vision overview Related Courses @ Stanford  CS131: Undergraduate introductory class cs224n: intersection of deep learning and natural language proces">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/1-13.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-20.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-26.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-31.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-40.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-41.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-44.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/2-59.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-14.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-17.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-34.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-42.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-46.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-49.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-53.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-57.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-65.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-73.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-76.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-82.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/3-83.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-8.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-23.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-43.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-45.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-50.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-75.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-77.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-79.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-86.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-88.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-94.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/4-96.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-27.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-31.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-36.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-40.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-62.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-64.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-65.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-73.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/5-75.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/6-44.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/6-46.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/6-47.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/6-59.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-9.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-16.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-61.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-68.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-70.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-71.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/7-93.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/8-39.jpg">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/8-50-pic-1.png">
<meta property="og:image" content="http://yoursite.com/assets/blogImg/cs231n/8-56-pic-1.png">
<meta property="og:updated_time" content="2018-10-25T06:44:57.008Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Convolutional Neural Networks for Visual Recognition|李飞飞">
<meta name="twitter:description" content="Lecture 1: Course Introduction [slice]1.1 Computer vision overview Related Courses @ Stanford  CS131: Undergraduate introductory class cs224n: intersection of deep learning and natural language proces">
<meta name="twitter:image" content="http://yoursite.com/assets/blogImg/cs231n/1-13.jpg">






  <link rel="canonical" href="http://yoursite.com/2018/09/28/video-cs231n-李飞飞/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Convolutional Neural Networks for Visual Recognition|李飞飞 | 玉琳的博客</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">玉琳的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">唯梦想与爱豆不可辜负</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br/>搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/09/28/video-cs231n-李飞飞/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="芸笙"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="玉琳的博客"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Convolutional Neural Networks for Visual Recognition|李飞飞

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-09-28 16:00:00" itemprop="dateCreated datePublished" datetime="2018-09-28T16:00:00+08:00">2018-09-28</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-25 14:44:57" itemprop="dateModified" datetime="2018-10-25T14:44:57+08:00">2018-10-25</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/学习笔记/" itemprop="url" rel="index"><span itemprop="name">学习笔记</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2018/09/28/video-cs231n-李飞飞/" class="leancloud_visitors" data-flag-title="Convolutional Neural Networks for Visual Recognition|李飞飞">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">阅读次数：</span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="Lecture-1-Course-Introduction-slice"><a href="#Lecture-1-Course-Introduction-slice" class="headerlink" title="Lecture 1: Course Introduction [slice]"></a>Lecture 1: Course Introduction <a href="https://drive.google.com/open?id=1x9x1YKG52ZA8X_YD4g68Sq85OvM_6bh-" target="_blank" rel="noopener">[slice]</a></h3><h4 id="1-1-Computer-vision-overview"><a href="#1-1-Computer-vision-overview" class="headerlink" title="1.1 Computer vision overview"></a>1.1 Computer vision overview</h4><blockquote>
<p>Related Courses @ Stanford</p>
<ul>
<li>CS131: Undergraduate introductory class</li>
<li>cs224n: intersection of deep learning and natural language processing</li>
<li>cs231a: more emcompassing</li>
<li>cs231n: algorithms and applications, CNN</li>
</ul>
</blockquote>
<h4 id="1-2-Historical-context"><a href="#1-2-Historical-context" class="headerlink" title="1.2 Historical context"></a>1.2 Historical context</h4><p><img src="/assets/blogImg/cs231n/1-13.jpg" width="700"></p>
<ul>
<li>if object recognition is too hard, do object segmentation firstly.</li>
<li>SIFT &amp; object recognition: use critical features to represent the whole object, and caculate two objects’ similarity.</li>
<li>PASCAL Visual Object Challenge</li>
</ul>
<h4 id="1-3-Course-logistics"><a href="#1-3-Course-logistics" class="headerlink" title="1.3 Course logistics"></a>1.3 Course logistics</h4><a id="more"></a>
<h3 id="Lecture-2-Image-Classification"><a href="#Lecture-2-Image-Classification" class="headerlink" title="Lecture 2: Image Classification"></a>Lecture 2: Image Classification</h3><h4 id="2-1-The-data-driven-approach"><a href="#2-1-The-data-driven-approach" class="headerlink" title="2.1 The data-driven approach"></a>2.1 The data-driven approach</h4><p>The Problem: Semantic Gap<br>像素值构成的数值矩阵与标签语义间的差别。稍微移动一下相机，标签不变，但像素值矩阵变化十分巨大，要求算法具有鲁棒性。 </p>
<p>解决方法1：基于规则<br>计算图形边缘，并对每部分进行分类，如三条线交于一点可能是猫耳朵。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify_image</span><span class="params">(image)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> class_label</span><br></pre></td></tr></table></figure></p>
<p>存在问题：</p>
<ul>
<li>容易出错</li>
<li>难以迁移到其他学习，例如狗、卡车等。</li>
</ul>
<p>解决方法2：数据驱动(Data-Driven Approach)</p>
<ol>
<li>Collect a dataset of images and labels</li>
<li>Use Machine Learning to train a classifier</li>
<li>Evaluate the classifier on new images<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(images, labels)</span>:</span></span><br><span class="line">    <span class="comment"># Machine learning</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(model, test_images)</span>:</span></span><br><span class="line">    <span class="comment"># Use model to predict labels</span></span><br><span class="line">    <span class="keyword">return</span> test_labels</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>First classifier: Nearest Neighbour<br>使用训练集中最像的image的label作为test集中需要预测的image的label<br>使用L1距离表示两张图片的相似度<br><img src="/assets/blogImg/cs231n/2-20.jpg" width="700"></p>
<blockquote>
<p>Example Dataset: CIFAR10</p>
</blockquote>
<p>源代码：<br><img src="/assets/blogImg/cs231n/2-26.jpg" width="700"><br>Complexity: train O(1), predict O(N)</p>
<p>Problem 1: 我们希望训练过程相对复杂而预测过程比较简单<br>解决办法：使用CNN等其他算法</p>
<p>problem 2: 由于离群点或噪声的存在，预测可能不准<br>解决办法：使用KNN(平滑边界)</p>
<h4 id="2-2-K-nearest-neighbor-notes"><a href="#2-2-K-nearest-neighbor-notes" class="headerlink" title="2.2 K-nearest neighbor [notes]"></a>2.2 K-nearest neighbor <a href="http://cs231n.github.io/classification" target="_blank" rel="noopener">[notes]</a></h4><p><img src="/assets/blogImg/cs231n/2-31.jpg" width="700"><br>L1距离和L2距离的差别：L1距离取决于坐标系，而L2距离与坐标系无关。<br>如果特征向量有明确的意义，用L1距离或许会更好；若特征向量只是某个空间中的一个通用向量，我们并不知道其意义，用L2距离或许会更好。<br><img src="/assets/blogImg/cs231n/2-40.jpg" width="700"><br><img src="/assets/blogImg/cs231n/2-41.jpg" width="700"><br>交叉验证更适用于较小的数据集，在深度学习中不常用。</p>
<p>KNN问题：</p>
<ul>
<li>不适用于图像处理</li>
<li>维度灾难(the curse of dimensionality)：随着特征维度增加，训练成本指数级增加。(维度高时，若想准确分类，需要更多的样本，否则样本在空间内分布稀疏)<br><img src="/assets/blogImg/cs231n/2-44.jpg" width="700"></li>
</ul>
<h4 id="2-3-Linear-classification-I-notes"><a href="#2-3-Linear-classification-I-notes" class="headerlink" title="2.3 Linear classification I [notes]"></a>2.3 Linear classification I <a href="http://cs231n.github.io/linear-classify" target="_blank" rel="noopener">[notes]</a></h4><p>问题1：对于某一种类别只会学习出单一的平均的模板，而对于图像而言，训练集常出现的是某个标准图像的各种变体，例如车的正面侧面，使得训练出的模板W变得似乎四不像。<br>解决方法：使用神经网络或者其他更为复杂的模型，不局限于仅仅学习一个模板(model)。<br>问题2：低维空间的线性不可分问题<br><img src="/assets/blogImg/cs231n/2-59.jpg" width="700"></p>
<h3 id="Lecture-3-Loss-Functions-and-Optimization"><a href="#Lecture-3-Loss-Functions-and-Optimization" class="headerlink" title="Lecture 3: Loss Functions and Optimization"></a>Lecture 3: Loss Functions and Optimization</h3><h4 id="3-1-Linear-classification-II"><a href="#3-1-Linear-classification-II" class="headerlink" title="3.1 Linear classification II"></a>3.1 Linear classification II</h4><p>通过损失函数Loss Function来确定较好的$W$。</p>
<blockquote>
<p>Given an example $(x_i,y_i)$, where $x_i$ is the image and $y_i$ is the (integer) label. The scores vector is $s=f(x_i,W)$<br>Loss over the dataset is a sum of loss over examples:</p>
<script type="math/tex; mode=display">W=argmin L=argmin \frac{1}{N}\sum L_i(f(x_i,W),y_i)</script><p>The SVM loss has the form:<br>\begin{align}<br>L_i &amp;= \sum\limits_{j\neq y_i}<br>    \begin{cases}<br>    0, &amp;\text{if $s_{y_i}\geq s_j+1$}\\<br>    s_j-s_{y_i}+1, &amp;\text{otherwise}<br>    \end{cases}\\<br>    &amp;=\sum\limits_{j\neq y_i}max(0,s_j-s_{y_i}+1)<br>\end{align}</p>
<p align="right">——"Hinge loss"</p>

</blockquote>
<p>对于所有错误的分类，比较正确分类的分数和错误分类的分数，如果正确分类的分数比错误分类的分数高出某个安全边界(此处设为1)，则损失设为0，否则设为两者之差。<br>threshold为1类似于SVM中的支持向量，无论设为多少，都是同时增大或减少。<br>Example:<br><img src="/assets/blogImg/cs231n/3-14.jpg" width="700"><br><img src="/assets/blogImg/cs231n/3-17.jpg" width="700"></p>
<p>Q1: What happens to loss if car scores change a bit?<br>A1: 由于car的分数比其他类别高出许多，所以即使图像稍微变动一点，Loss Function依然会是0。因而这个模型比较具有鲁棒性。 </p>
<p>Q2: what is the min/max possible loss?<br>A2: 根据Hinge loss，最大可能值为0，最小可能值为负无穷。</p>
<p>Q3: At initialization W is small so all s ≈ 0. What is the loss?<br>A3: $L_i = \sum\limits_{j\neq y_i}1=N-1$</p>
<p>Q4: What if the sum was over all classes? (including j = y_i)<br>A4: 结果+1</p>
<p>Q5: What if we used mean instead of sum?<br>A5: 损失函数缩小常数倍，结果不影响。</p>
<p>Q6: What if we used $L_i=\sum\limits_{j\neq y_i}max(0,s_j-s_{y_i}+1)^2$?<br>A6: 损失函数进行了非线性变换，结果会产生影响。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i_vectorized</span><span class="params">(x,y,W)</span>:</span></span><br><span class="line">    scores = W.dot(x)</span><br><span class="line">    margins = np.maximum(<span class="number">0</span>, scores - scores[y] + <span class="number">1</span>)</span><br><span class="line">    margins[y] = <span class="number">0</span></span><br><span class="line">    loss_i = np.sum(margins)</span><br><span class="line">    <span class="keyword">return</span> loss_i</span><br></pre></td></tr></table></figure>
<p>Q7: Suppose that we found a W such that L = 0. Is this W unique?<br>A7: Not unique. For any constant c$\neq$0,cW also satisfies.</p>
<p>避免过拟合，加入正则项，选择更简单的W。L1偏向于更稀疏，L2偏向于所有数字更小。<br><img src="/assets/blogImg/cs231n/3-34.jpg" width="700"></p>
<h4 id="3-2-Higher-level-representations-image-features"><a href="#3-2-Higher-level-representations-image-features" class="headerlink" title="3.2 Higher-level representations, image features"></a>3.2 Higher-level representations, image features</h4><p>Softmax Classifier(Multinomial Logistic Regression)<br>对于SVM模型，我们并没有指定scores的内在含义，而对于Softmax loss，scores表示概率。我们希望真实标签的概率越大且接近1。<br><img src="/assets/blogImg/cs231n/3-42.jpg" width="700"><br>Example:<br><img src="/assets/blogImg/cs231n/3-46.jpg" width="700"></p>
<p>Q1: What is the min/max possible loss L_i?<br>A1: 最大0，最小负无穷。</p>
<p>Q2: Usually at initialization W is small ao all s ≈ 0. What is the loss?<br>A2: $\log{N}$</p>
<p>SVM &amp; Softmax对比：<br><img src="/assets/blogImg/cs231n/3-49.jpg" width="700"></p>
<p>Conclusion:<br><img src="/assets/blogImg/cs231n/3-53.jpg" width="700"></p>
<h4 id="3-3-Optimization-stochastic-gradient-descent"><a href="#3-3-Optimization-stochastic-gradient-descent" class="headerlink" title="3.3 Optimization, stochastic gradient descent"></a>3.3 Optimization, stochastic gradient descent</h4><p>Strategy 1. random search<br><img src="/assets/blogImg/cs231n/3-57.jpg" width="700"></p>
<p>Strategy 2. Follow the slope(gradient descent)</p>
<ul>
<li><p>finite difference:<br><img src="/assets/blogImg/cs231n/3-65.jpg" width="700"></p>
</li>
<li><p>Calculus<br><img src="/assets/blogImg/cs231n/3-73.jpg" width="700"></p>
</li>
</ul>
<p>Problem: 当N，即样本量很大时，每次迭代每个变量的梯度会使得计算十分复杂。此时经常采用随机梯度下降法。<br>Stochastic Gradient Descent<br><img src="/assets/blogImg/cs231n/3-76.jpg" width="700"><br>其中256是一个minibatch，总是使用形如$2^n$作为minibatch。</p>
<h4 id="并没有听懂"><a href="#并没有听懂" class="headerlink" title="[并没有听懂]"></a>[并没有听懂]</h4><p><img src="/assets/blogImg/cs231n/3-82.jpg" width="700"><br><img src="/assets/blogImg/cs231n/3-83.jpg" width="700"></p>
<h3 id="Lecture-4-Introduction-to-Neural-Networks"><a href="#Lecture-4-Introduction-to-Neural-Networks" class="headerlink" title="Lecture 4: Introduction to Neural Networks"></a>Lecture 4: Introduction to Neural Networks</h3><h4 id="4-1-Backpropagation"><a href="#4-1-Backpropagation" class="headerlink" title="4.1 Backpropagation"></a>4.1 Backpropagation</h4><p>computational graphs<br><img src="/assets/blogImg/cs231n/4-8.jpg" width="700"></p>
<ol>
<li>express a function using a computational graph</li>
<li>use Backpropagation to compute gradient(recursively use the chain rule)</li>
</ol>
<h4 id="4-2-Multi-layer-Perceptrons"><a href="#4-2-Multi-layer-Perceptrons" class="headerlink" title="4.2 Multi-layer Perceptrons"></a>4.2 Multi-layer Perceptrons</h4><p>Examples:<br><img src="/assets/blogImg/cs231n/4-23.jpg" width="700"><br><img src="/assets/blogImg/cs231n/4-43.jpg" width="700"><br>当某个较为复杂的部分可以计算梯度时，可以选择用一个节点去替代它，如上图例二中的sigmoid函数。<br><img src="/assets/blogImg/cs231n/4-45.jpg" width="700"></p>
<p><img src="/assets/blogImg/cs231n/4-50.jpg" width="700"><br>add: 按本身值分配<br>max: 分配给最大值，其余节点为0<br>multiply: 按另一个节点的值分配给当前节点</p>
<blockquote>
<p>Always check: The gradient with respect to a variable should have the same shape as the variable.</p>
</blockquote>
<p>Codes:<br><img src="/assets/blogImg/cs231n/4-75.jpg" width="700"><br><img src="/assets/blogImg/cs231n/4-77.jpg" width="700"><br><img src="/assets/blogImg/cs231n/4-79.jpg" width="700"></p>
<h4 id="4-3-The-neural-viewpoint"><a href="#4-3-The-neural-viewpoint" class="headerlink" title="4.3 The neural viewpoint"></a>4.3 The neural viewpoint</h4><p><img src="/assets/blogImg/cs231n/4-86.jpg" width="700"><br>Code:<br><img src="/assets/blogImg/cs231n/4-88.jpg" width="700"></p>
<p>神经网络的工作原理:<br><img src="/assets/blogImg/cs231n/4-94.jpg" width="700"></p>
<p>Activation functions:<br><img src="/assets/blogImg/cs231n/4-96.jpg" width="700"></p>
<h3 id="Lecture-5-Convolutional-Neural-Networks"><a href="#Lecture-5-Convolutional-Neural-Networks" class="headerlink" title="Lecture 5: Convolutional Neural Networks"></a>Lecture 5: Convolutional Neural Networks</h3><h4 id="5-1-History"><a href="#5-1-History" class="headerlink" title="5.1 History"></a>5.1 History</h4><ul>
<li>CNN需要训练卷积层，卷积层能够更好地保留输入的空间结构</li>
</ul>
<h4 id="5-2-Convolution-and-pooling"><a href="#5-2-Convolution-and-pooling" class="headerlink" title="5.2 Convolution and pooling"></a>5.2 Convolution and pooling</h4><p>Fully Connected Layer:<br><img src="/assets/blogImg/cs231n/5-27.jpg" width="700"> </p>
<p>Convolution Layer<br><img src="/assets/blogImg/cs231n/5-31.jpg" width="700"><br>将卷积核和原层对应部分做点乘再相加。<br>[32<em>32</em>3]x[5<em>5</em>3] —&gt; [28<em>28</em>1]<br>使用不同的卷积核，可以得到多个activation maps<br><img src="/assets/blogImg/cs231n/5-36.jpg" width="700"> </p>
<p>Example:<br><img src="/assets/blogImg/cs231n/5-40.jpg" width="700"> </p>
<p>output size: (N - F) / stride + 1<br>Common to zero pad the border: (N + pad_N * 2 - F) / stride + 1<br>零填充的原因：保证全尺寸输出 —&gt; stride=1时，填充(F-1)/2</p>
<blockquote>
<p>Examples:<br>Input volume: [32<em>32</em>3]<br>filters: 10 [5<em>5</em>3] with stride1, pad 2<br>Q1: What’s the output volume size?<br>output size = (32+2<em>2-5)/1+1=32<br>so, output volume size = 32</em>32<em>10<br>Q2: Number of parameters in this layer?<br>Each filter has 5</em>5<em>3+1=76 params, one for bias,<br>so total number of parameters is 76</em>10=760</p>
</blockquote>
<p>summary:<br><img src="/assets/blogImg/cs231n/5-62.jpg" width="700"> </p>
<p>Example: CONV layers in Torch and Caffe<br><img src="/assets/blogImg/cs231n/5-64.jpg" width="700"><br><img src="/assets/blogImg/cs231n/5-65.jpg" width="700"> </p>
<h4 id="5-3-ConvNets-outside-vision"><a href="#5-3-ConvNets-outside-vision" class="headerlink" title="5.3 ConvNets outside vision"></a>5.3 ConvNets outside vision</h4><p>Pooling: 池化<br>目的：降维，减少参数数量；深度不减<br>example: [224<em>224</em>64] —&gt; [112<em>112</em>64]<br><img src="/assets/blogImg/cs231n/5-73.jpg" width="700"><br>max polling 可以表示当前卷积核所在位置的最大受激活程度<br>一般不在池化层填0，因为池化层一般只做降采样<br><img src="/assets/blogImg/cs231n/5-75.jpg" width="700"><br>最后添加一个全连接层和朴素的神经网络相连</p>
<p>demo: <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" rel="noopener">http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html</a></p>
<h3 id="Lecture-6-Training-Neural-Networks-part-I"><a href="#Lecture-6-Training-Neural-Networks-part-I" class="headerlink" title="Lecture 6: Training Neural Networks, part I"></a>Lecture 6: Training Neural Networks, part I</h3><h4 id="6-1-Activation-functions"><a href="#6-1-Activation-functions" class="headerlink" title="6.1 Activation functions"></a>6.1 Activation functions</h4><blockquote>
<p><strong>1. 画computional graphs</strong><br>函数：$f=Wx$<br>hinge loss: $L_i=\sum\limits_{j\neq y_i}max(0,s_j-s_{y_i}+1)<br>penalty: $R(W)$<br><strong>2. 关于$f$</strong><br>Neural Networks<br>Linear score function: $f=Wx$<br>2-layer Neural Network: $f=W_2max(0,W_1x)$<br>包含若干个线性层，层与层之间通过非线性函数连接实现层的堆叠<br><strong>3. 一种特殊的神经网络—CNN</strong><br>使用卷积层保持输入的空间结构，卷积层是一个权重滤波器，每个权重输出一个激活图，重复产生多个激活图<br><strong>4. 通过优化来选择权重W</strong><br>向负梯度方向更新 mini-batch SGD<br>Loop:<br><strong>Sample</strong> a batch of data<br><strong>Forward</strong> prop it through the graph (network), get loss<br><strong>Backprop</strong> to calculate the gradients<br><strong>Update</strong> the parameters using the gradient<br><strong>5. 训练神经网络</strong></p>
<ul>
<li><strong>One time setup</strong><br>activation functions, preprocessing, weight initialization, regularization, gradient checking</li>
<li><strong>Training dynamics</strong><br>babysitting the learning process, parameter updates, hyperparameter optimization</li>
<li><strong>Evaluation</strong><br>model ensembles</li>
</ul>
</blockquote>
<h5 id="6-1-1-Sigmoid"><a href="#6-1-1-Sigmoid" class="headerlink" title="6.1.1 Sigmoid"></a>6.1.1 Sigmoid</h5><p>将数字压缩到(0,1)<br>问题：</p>
<ul>
<li>当X是很大的正数或很大的负数时，值会很接近1或0，此时更新梯度变化很小，导致梯度消失</li>
<li>Sigmoid函数的输出不是零中心的，若初始X全为正，梯度也会全为正，这样梯度只会朝向一三象限。</li>
<li>exp()计算代价较高</li>
</ul>
<h5 id="6-1-2-tanh"><a href="#6-1-2-tanh" class="headerlink" title="6.1.2 tanh"></a>6.1.2 tanh</h5><p>将数字压缩到[-1,1]<br>优点：零中心<br>缺点：当X是很大的正数或很大的负数时，值会很接近1或0，此时更新梯度变化很小，导致梯度消失</p>
<h5 id="6-1-2-ReLU"><a href="#6-1-2-ReLU" class="headerlink" title="6.1.2 ReLU"></a>6.1.2 ReLU</h5><p>$f(x)=max(0,x)$<br>优点：在正轴上不会饱和；计算快；收敛地更快；更具备生物合理性(实证检验)<br>缺点：不是零中心，对于负半轴会出现梯度消失<br>于是人们总喜欢用一些较小的正偏置项来初始化ReLU</p>
<h5 id="6-1-3-Leaky-ReLU"><a href="#6-1-3-Leaky-ReLU" class="headerlink" title="6.1.3 Leaky ReLU"></a>6.1.3 Leaky ReLU</h5><p>$f(x)=max(0.01x,x)$<br>存在负斜率，在负半轴结果也较好，不会存在梯度消失情况</p>
<h5 id="6-1-4-PReLU-Parametric"><a href="#6-1-4-PReLU-Parametric" class="headerlink" title="6.1.4 PReLU(Parametric)"></a>6.1.4 PReLU(Parametric)</h5><p>$f(x)=max(\alpha x,x)$<br>将$\alpha$设置为一个超参数，通过学习得到，增加了灵活性</p>
<h5 id="6-1-5-Exponential-Linear-Units-ELU"><a href="#6-1-5-Exponential-Linear-Units-ELU" class="headerlink" title="6.1.5 Exponential Linear Units(ELU)"></a>6.1.5 Exponential Linear Units(ELU)</h5><p>$f(x) =<br>    \begin{cases}<br>    0, &amp;\text{if $x&gt;0$}\\<br>    \alpha (exp(x)-1), &amp;\text{if $x\leq 0$}<br>    \end{cases}$<br>使得均值更接近于0，但是使得负半轴的非系统偏差更容易被保留<br>缺点：exp()计算复杂 </p>
<h5 id="6-1-6-Maxout-“Neuron”"><a href="#6-1-6-Maxout-“Neuron”" class="headerlink" title="6.1.6 Maxout “Neuron”"></a>6.1.6 Maxout “Neuron”</h5><p>$f(x)=max(x_1^Tx+b_1,x_2^Tx+b_2)$<br>问题：参数个数加倍</p>
<blockquote>
<p>TLDR: In practice: </p>
<ul>
<li>Use ReLU. Be careful with your learning rates </li>
<li>Try out Leaky ReLU / Maxout / ELU </li>
<li>Try out tanh but don’t expect much</li>
<li>Don’t use sigmoid</li>
</ul>
</blockquote>
<h4 id="6-2-Data-Preprocessing"><a href="#6-2-Data-Preprocessing" class="headerlink" title="6.2 Data Preprocessing"></a>6.2 Data Preprocessing</h4><p>对数据-样本均值，变成零中心数据，这样在第一层输入解决了零均值问题，然而在之后的层中，非零均值的问题更加严重了，所以还是没有解决sigmiod的问题。</p>
<h4 id="6-3-Weight-Initialization"><a href="#6-3-Weight-Initialization" class="headerlink" title="6.3 Weight Initialization"></a>6.3 Weight Initialization</h4><p>First idea: W=np.random.randn(fan_in,fan_out)*0.01<br><img src="/assets/blogImg/cs231n/6-44.jpg" width="700"><br><img src="/assets/blogImg/cs231n/6-46.jpg" width="700"><br>每一层均值基本为0，方差越来越小<br>问题：最后所有激活值都变成了0</p>
<p>Second Idea: np.random.randn(fan_in,fan_out)*1.0 如果改变初始化比例的大小，将0.01改成1.0，则W中的数总是过大，导致模型总是处于饱和状态(趋于+1/-1)，从而使得所有的梯度趋于0而不再更新<br><img src="/assets/blogImg/cs231n/6-47.jpg" width="700"></p>
<p>Third Idea: “Xavier initialization”<br>W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in)<br>即单位高斯权值<br>问题：如果使用ReLU，由于将一半的数调整为了0，方差也减小了一半，于是数据又会趋向于0</p>
<p>Fourth Idea: note additional /2<br>W=np.random.randn(fan_in,fan_out)/np.sqrt(fan_in/2)</p>
<h4 id="6-4-batch-normalization"><a href="#6-4-batch-normalization" class="headerlink" title="6.4 batch normalization"></a>6.4 batch normalization</h4><p>对每层输入的X做归一化<br>目的：为了避免饱和<br>问题：是否必要？<br>解决：控制饱和程度，在归一化之后，有时通过缩放恢复原函数并做一个小的扰动</p>
<p><img src="/assets/blogImg/cs231n/6-59.jpg" width="700"><br>好处：改善了梯度流，更加robust</p>
<h4 id="6-5-Babysitting-the-Learning-Process"><a href="#6-5-Babysitting-the-Learning-Process" class="headerlink" title="6.5 Babysitting the Learning Process"></a>6.5 Babysitting the Learning Process</h4><ol>
<li>Preprocess the data: zero-centered/normalized</li>
<li>choose the architecture: hyperparameters of hidden layer neurons</li>
<li>initialize the network, make sure the loss is reasonable(sanity check): 当权重很小，分布很分散时，归一化指数分类器的损失将是$-\log 1/N$</li>
<li>加入正则项</li>
<li>从小的trainset开始训练，测试模型正确性，去掉正则项，查看权重是否逐渐降为0，准确度逐渐升至1</li>
<li>加入小的正则项，确定learning rate [1e-5,1e-3]</li>
</ol>
<h4 id="6-6-Hyperparameter-Optimization"><a href="#6-6-Hyperparameter-Optimization" class="headerlink" title="6.6 Hyperparameter Optimization"></a>6.6 Hyperparameter Optimization</h4><p>cross-validation strategy<br>random search vs grid search<br>好的learning rate 应该类似于loss先急剧下降，再缓慢下降</p>
<h3 id="Lecture-7-Training-Neural-Networks-part-II"><a href="#Lecture-7-Training-Neural-Networks-part-II" class="headerlink" title="Lecture 7: Training Neural Networks, part II"></a>Lecture 7: Training Neural Networks, part II</h3><p>Recall:<br>Weight Initialization: 权重过大则爆炸式增长，权重过小则梯度消失，使用Xaiver<br>对于越深的网络，权重初始化越重要，因为后续的过程中会不断的乘以那些矩阵<br>归一化: 使得数据对参数扰动的变化没有那么明显，比较robust<br><img src="/assets/blogImg/cs231n/7-9.jpg" width="700"></p>
<h4 id="7-1-Fancier-optimization"><a href="#7-1-Fancier-optimization" class="headerlink" title="7.1 Fancier optimization"></a>7.1 Fancier optimization</h4><h5 id="7-1-1-SGD"><a href="#7-1-1-SGD" class="headerlink" title="7.1.1: SGD"></a>7.1.1: SGD</h5><p>每次向负梯度方向更新<br>$x_{t+1}=x_t-\alpha *\nabla f(x_t)$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  x += learning_rate * dx</span><br></pre></td></tr></table></figure></p>
<p>Problem 1: 损失函数在某些方向非常敏感，而在某些方向天然地不敏感<br><img src="/assets/blogImg/cs231n/7-16.jpg" width="700"></p>
<p>Problem 2: if the loss function has a local minima or saddle point, since there’s zero gradient, gradient descent gets stuck<br>对于低维情况，似乎local minima问题更严重，但对于高维情况，saddle point相对来说更常见</p>
<p>Problem 3: our gradients come from minibatches so they can be noisy. 当前点对梯度的估计由于参数的随机性等包含了较大的噪声</p>
<h5 id="7-1-2-SGD-Momentum"><a href="#7-1-2-SGD-Momentum" class="headerlink" title="7.1.2: SGD+Momentum"></a>7.1.2: SGD+Momentum</h5><p>想法类似于一个小球滚落，当经过局部最优点或者鞍点的时候，由于速度为0，则会停止在当前点，但如果给了一个速度，可能能越过局部最小点，鞍点处更容易突破；同时，由于保留了上次的速度方向，曲折回合的更新趋势会有所抵消，使得梯度能更快地收敛到最优解<br>$v_{t+1}=\rho v_t+\nabla f(x_t)$<br>$x_{t+1}=x_t-\alpha v_{t+1}$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  vx = rho * vx + dx</span><br><span class="line">  x += learning_rate * dx</span><br></pre></td></tr></table></figure></p>
<h5 id="7-1-3-Nesterov"><a href="#7-1-3-Nesterov" class="headerlink" title="7.1.3 Nesterov"></a>7.1.3 Nesterov</h5><p>$v_{t+1}=\rho v_t-\alpha\nabla f(x_t+\rho v_t)$<br>$x_{t+1}=x_t + v_{t+1}$<br>$\Rightarrow$<br>$v_{t+1}=\rho v_t-\alpha\nabla f(\tilde{x}_t)$<br>$\tilde{x}_{t+1}=\tilde{x}_t-\rho v_t+(1+\rho)v_{t+1}=\tilde{x}_t+v_{t+1}+\rho(v_{t+1}-v_t)$<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  vx = rho * vx + dx</span><br><span class="line">  x += learning_rate * dx</span><br></pre></td></tr></table></figure></p>
<h5 id="7-1-4-AdaGrad"><a href="#7-1-4-AdaGrad" class="headerlink" title="7.1.4 AdaGrad"></a>7.1.4 AdaGrad</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  grad_squared += dx * dx</span><br><span class="line">  x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>对于小梯度，除的数也较小，使得学习速率相当于变大；反之，对于大梯度，除的数也较大，使得学习速率相当于变小。<br>Problem: 随着时间增长，grad_squared会逐渐累积，使得最后步长会越来越小，接近于0，在凸函数的情况下表现很好，非凸问题会困于局部最优点</p>
<h5 id="7-1-5-RMDProp"><a href="#7-1-5-RMDProp" class="headerlink" title="7.1.5 RMDProp"></a>7.1.5 RMDProp</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  grad_squared = decay_rate &amp; grad_squared + (<span class="number">1</span> - decay_rate) * dx * dx</span><br><span class="line">  x -= learning_rate * dx / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure>
<p>梯度平方按照一定比例下降，好处在于步长最后趋近于0被修正了，坏处在于训练会越来越慢</p>
<h5 id="7-1-6-Adam-almost"><a href="#7-1-6-Adam-almost" class="headerlink" title="7.1.6: Adam(almost)"></a>7.1.6: Adam(almost)</h5><p>将以上动量和除平方方法结合起来<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  first_moment = beta1 * first_moment + (<span class="number">1</span> - beta1) * dx <span class="comment"># Momentum</span></span><br><span class="line">  second_moment =  beta2 * second_moment + (<span class="number">1</span> - beta2) * dx * dx <span class="comment"># AdaGrad/RMSProp</span></span><br><span class="line">  x -= learning_rate * first_moment / (np.sqrt(second_moment) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></p>
<p>Problem: 第一步时second_moment仍然接近0，处以其根号，得很大的值，即开始时步长非常大</p>
<h5 id="7-1-7-Adam-full-form"><a href="#7-1-7-Adam-full-form" class="headerlink" title="7.1.7: Adam(full form)"></a>7.1.7: Adam(full form)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">first_moment = <span class="number">0</span></span><br><span class="line">second_moment = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  first_moment = beta1 * first_moment + (<span class="number">1</span> - beta1) * dx <span class="comment"># Momentum</span></span><br><span class="line">  second_moment =  beta2 * second_moment + (<span class="number">1</span> - beta2) * dx * dx <span class="comment"># AdaGrad/RMSProp</span></span><br><span class="line">  first_unbias = first_moment / (<span class="number">1</span> - beta ** t) <span class="comment"># Bias correction</span></span><br><span class="line">  second_unbias = second_moment / (<span class="number">1</span> - beta2 ** t) <span class="comment"># Bias correction</span></span><br><span class="line">  x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + <span class="number">1e-7</span>) <span class="comment"># AdaGrad/RMSProp</span></span><br></pre></td></tr></table></figure>
<p><strong>Adam 效果总是非常好，总是首选使用。默认设置beta1=0.9, beta2=0.999, learning_rate=1e-3 or 5e-4</strong></p>
<p>选择一个好的learning rate<br>we can let learning rate decay over time<br>exponential decay: $\alpha=\alpha_0e^{-kt}$<br>1/t decay: $\alpha = \alpha_0/(1+kt)$</p>
<h5 id="7-1-8-Newton-method"><a href="#7-1-8-Newton-method" class="headerlink" title="7.1.8 Newton method"></a>7.1.8 Newton method</h5><p>$\theta^*=\theta_0-H^{-1}\nabla_{\theta}J(\theta_0)$<br>使用二阶逼近，没有使用learning_rate，现实中可能使用learning_rate<br>Problem: 需要求Hessian matrix，对于深度学习有些不切实际<br>Quasi-Newton methods: 不求矩阵的逆，用低阶逼近</p>
<h4 id="7-2-Model-Ensembles"><a href="#7-2-Model-Ensembles" class="headerlink" title="7.2 Model Ensembles"></a>7.2 Model Ensembles</h4><p>Idea 1: 独立学习多个学习器，最后组合到一起，例如求平均，最大最小等，可以减少过拟合，达到小但固定的提升</p>
<p>Idea 2: Tips &amp; Tricks<br>Instead of using actual parameter vector, keep a moving average of the parameter vector and use that<br>at test time (Polyak averaging)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  data_batch = dataset.sample_data_batch()</span><br><span class="line">  loss = network.forward(data_batch)</span><br><span class="line">  dx = network.backward()</span><br><span class="line">  x -= learning_rate * dx </span><br><span class="line">  x_test = <span class="number">0.005</span> * x_test + <span class="number">0.005</span> * x</span><br></pre></td></tr></table></figure></p>
<h4 id="7-3-Regulation"><a href="#7-3-Regulation" class="headerlink" title="7.3 Regulation"></a>7.3 Regulation</h4><p>用于提升单个学习器的表现<br>Idea 1: L1, L2, Elastic</p>
<p>Idea 2: 对于NN：Dropout<br>即在正向传播过程中，对于每层激活函数随机选取一些神经元置零，一般使用在全连接层。<br><img src="/assets/blogImg/cs231n/7-61.jpg" width="700"><br>测试时引入Dropout不好，会改变预测结果，考虑对训练的不同情况做平均<br><img src="/assets/blogImg/cs231n/7-68.jpg" width="700"><br><img src="/assets/blogImg/cs231n/7-70.jpg" width="700"><br>Inverted Dropout: test时消除了p，测试时更加高效<br><img src="/assets/blogImg/cs231n/7-71.jpg" width="700"></p>
<p>Idea 3: Batch Normalization<br>使用最多</p>
<p>Idea 4: Data augmentation<br>将原始数据做一定的变换，标签不变。例如对于原始图像，随机地加个滤镜或者翻转旋转或者局部裁剪，标签仍设为猫，增加样本数</p>
<p>Idea 5: DropConnect<br>即在正向传播过程中，对于每层权重矩阵置零</p>
<p>Idea 6: fractional max pooling</p>
<p>Idea 7: Stochastic Depth<br>随机消除一些层，直接连向下一层</p>
<h4 id="7-4-Transfer-Learning"><a href="#7-4-Transfer-Learning" class="headerlink" title="7.4 Transfer Learning"></a>7.4 Transfer Learning</h4><p>处理过拟合问题，一种方法是使用正则化，一种方法是使用迁移学习</p>
<ol>
<li>在一个已知的大训练集上训练的一个较好的模型，如Imagenet</li>
<li>对于小的数据集，对不需要的类别的权重进行固定，训练那些需要的类别对应的权重。例如Imagenet中有3000类，需要进行10种狗的训练，W=[4096<em>10]-d，则将另外[4096</em>2990]-d固定。</li>
<li>对于更大的数据集，可能只需要微调，所以对于这个问题，将learning_rate调低，再次训练即可。</li>
</ol>
<p><img src="/assets/blogImg/cs231n/7-93.jpg" width="700"></p>
<h3 id="Lecture-8-Deep-Learning-Software"><a href="#Lecture-8-Deep-Learning-Software" class="headerlink" title="Lecture 8: Deep Learning Software"></a>Lecture 8: Deep Learning Software</h3><h4 id="Computational-Graphs"><a href="#Computational-Graphs" class="headerlink" title="Computational Graphs"></a>Computational Graphs</h4><p>Numpy VS TensorFlow VS PyTorch<br><img src="/assets/blogImg/cs231n/8-39.jpg" width="700"></p>
<h4 id="TensorFlow-Neural-Network"><a href="#TensorFlow-Neural-Network" class="headerlink" title="TensorFlow: Neural Network"></a>TensorFlow: Neural Network</h4><p>Running example: Train a two-layer ReLU network on random data with L2 loss<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">N, D, H = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(N, D))</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(N, D))</span><br><span class="line">w1 = tf.placeholder(tf.float32, shape=(D, H))</span><br><span class="line">w2 = tf.placeholder(tf.float32, shape=(H, D))</span><br><span class="line"></span><br><span class="line">h = tf.maximum(tf.matmul(x, w1), <span class="number">0</span>)</span><br><span class="line">y_pred = tf.matmul(h, w2)</span><br><span class="line">diff = y_pred - y</span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(diff ** <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    values = &#123;</span><br><span class="line">        x: np.random.randn(N, D),</span><br><span class="line">        w1: np.random.randn(D, H),</span><br><span class="line">        w2: np.random.randn(H, D),</span><br><span class="line">        y: np.random.randn(N, D),</span><br><span class="line">    &#125;</span><br><span class="line">    learning_rate = <span class="number">1e-5</span></span><br><span class="line">    t_list = []</span><br><span class="line">    loss_val_list = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        out = sess.run([loss, grad_w1, grad_w2], feed_dict=values)</span><br><span class="line">        loss_val, grad_w1_val, grad_w2_val = out</span><br><span class="line">        values[w1] -= learning_rate * grad_w1_val</span><br><span class="line">        values[w2] -= learning_rate * grad_w2_val</span><br><span class="line">        t_list.append(t)</span><br><span class="line">        loss_val_list.append(loss_val)</span><br><span class="line">    plt.scatter(t_list, loss_val_list)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>result:<br><img src="/assets/blogImg/cs231n/8-50-pic-1.png" width="700"></p>
<p>问题1：权重矩阵每次都在TensorFlow和Numpy中复制，导致资源浪费<br>解决方案：将placeholder改为Variable<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">N, D, H = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(N, D))</span><br><span class="line">y = tf.placeholder(tf.float32, shape=(N, D))</span><br><span class="line"><span class="comment"># Change w1 and w2 from placeholder to Variable</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal((D, H)))</span><br><span class="line">w2 = tf.Variable(tf.random_normal((H, D)))</span><br><span class="line"></span><br><span class="line">h = tf.maximum(tf.matmul(x, w1), <span class="number">0</span>)</span><br><span class="line">y_pred = tf.matmul(h, w2)</span><br><span class="line">diff = y_pred - y</span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(diff ** <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line">grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])</span><br><span class="line"></span><br><span class="line"><span class="comment">#  update w1 and w2 as part of the graph</span></span><br><span class="line">learning_rate = <span class="number">1e-5</span></span><br><span class="line">new_w1 = w1.assign(w1 - learning_rate * grad_w1)</span><br><span class="line">new_w2 = w2.assign(w2 - learning_rate * grad_w2)</span><br><span class="line">updates = tf.group(new_w1, new_w2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    values = &#123;</span><br><span class="line">        x: np.random.randn(N, D),</span><br><span class="line">        y: np.random.randn(N, D),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    t_list = []</span><br><span class="line">    loss_val_list = []</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        loss_val, _ = sess.run([loss, updates], feed_dict=values)</span><br><span class="line">        t_list.append(t)</span><br><span class="line">        loss_val_list.append(loss_val)</span><br><span class="line">    plt.scatter(t_list, loss_val_list)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>result:<br><img src="/assets/blogImg/cs231n/8-56-pic-1.png" width="700"></p>
<h4 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h4><p>Computational Graphs: static vs dynamic</p>
<ol>
<li>静态在后续可以多次更新，寻找最优值 (static)</li>
<li>一旦建立好，可以序列化，不需要每次都重新建立 (static)</li>
<li>有时可以让代码更简洁(dynamic)<br>TensorFlow: static; PyTorch: dynamic</li>
</ol>
<p>Dynamic Graph Applications</p>
<ul>
<li>Recurrent Networks</li>
<li>Recursive Networks</li>
<li>Modular Networks</li>
</ul>
<h4 id="Caffe"><a href="#Caffe" class="headerlink" title="Caffe"></a>Caffe</h4><h3 id="Lecture-9-CNN-Architectures"><a href="#Lecture-9-CNN-Architectures" class="headerlink" title="Lecture 9: CNN Architectures"></a>Lecture 9: CNN Architectures</h3><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><h3 id="Lecture-10-RNN"><a href="#Lecture-10-RNN" class="headerlink" title="Lecture 10: RNN"></a>Lecture 10: RNN</h3><h3 id="Lecture-11-Applications"><a href="#Lecture-11-Applications" class="headerlink" title="Lecture 11: Applications"></a>Lecture 11: Applications</h3><h4 id="11-1-分割"><a href="#11-1-分割" class="headerlink" title="11.1 分割"></a>11.1 分割</h4><p>Semantic Segmentation Idea: Fully Convolutional<br>Semantic Segmentation Idea: Fully Convolutional</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/23/training-camp-machine-learning-task-list/" rel="next" title="机器学习训练营|任务清单">
                <i class="fa fa-chevron-left"></i> 机器学习训练营|任务清单
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/10/09/LintCode-0174-remove-nth-node-from-end-of-list/" rel="prev" title="LintCode第174题|删除链表中倒数第n个节点 [简单]">
                LintCode第174题|删除链表中倒数第n个节点 [简单] <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="芸笙"/>
            
              <p class="site-author-name" itemprop="name">芸笙</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">29</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-1-Course-Introduction-slice"><span class="nav-number">1.</span> <span class="nav-text">Lecture 1: Course Introduction [slice]</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Computer-vision-overview"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 Computer vision overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-Historical-context"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 Historical context</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-Course-logistics"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 Course logistics</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-2-Image-Classification"><span class="nav-number">2.</span> <span class="nav-text">Lecture 2: Image Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-The-data-driven-approach"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 The data-driven approach</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-K-nearest-neighbor-notes"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 K-nearest neighbor [notes]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Linear-classification-I-notes"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 Linear classification I [notes]</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-3-Loss-Functions-and-Optimization"><span class="nav-number">3.</span> <span class="nav-text">Lecture 3: Loss Functions and Optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Linear-classification-II"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 Linear classification II</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Higher-level-representations-image-features"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 Higher-level representations, image features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-Optimization-stochastic-gradient-descent"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 Optimization, stochastic gradient descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#并没有听懂"><span class="nav-number">3.4.</span> <span class="nav-text">[并没有听懂]</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-4-Introduction-to-Neural-Networks"><span class="nav-number">4.</span> <span class="nav-text">Lecture 4: Introduction to Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Backpropagation"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 Backpropagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Multi-layer-Perceptrons"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 Multi-layer Perceptrons</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-The-neural-viewpoint"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 The neural viewpoint</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-5-Convolutional-Neural-Networks"><span class="nav-number">5.</span> <span class="nav-text">Lecture 5: Convolutional Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-History"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 History</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-Convolution-and-pooling"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 Convolution and pooling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-ConvNets-outside-vision"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 ConvNets outside vision</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-6-Training-Neural-Networks-part-I"><span class="nav-number">6.</span> <span class="nav-text">Lecture 6: Training Neural Networks, part I</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-Activation-functions"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 Activation functions</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-1-Sigmoid"><span class="nav-number">6.1.1.</span> <span class="nav-text">6.1.1 Sigmoid</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-2-tanh"><span class="nav-number">6.1.2.</span> <span class="nav-text">6.1.2 tanh</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-2-ReLU"><span class="nav-number">6.1.3.</span> <span class="nav-text">6.1.2 ReLU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-3-Leaky-ReLU"><span class="nav-number">6.1.4.</span> <span class="nav-text">6.1.3 Leaky ReLU</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-4-PReLU-Parametric"><span class="nav-number">6.1.5.</span> <span class="nav-text">6.1.4 PReLU(Parametric)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-5-Exponential-Linear-Units-ELU"><span class="nav-number">6.1.6.</span> <span class="nav-text">6.1.5 Exponential Linear Units(ELU)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-1-6-Maxout-“Neuron”"><span class="nav-number">6.1.7.</span> <span class="nav-text">6.1.6 Maxout “Neuron”</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-Data-Preprocessing"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 Data Preprocessing</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-Weight-Initialization"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 Weight Initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-batch-normalization"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 batch normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-Babysitting-the-Learning-Process"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 Babysitting the Learning Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-6-Hyperparameter-Optimization"><span class="nav-number">6.6.</span> <span class="nav-text">6.6 Hyperparameter Optimization</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-7-Training-Neural-Networks-part-II"><span class="nav-number">7.</span> <span class="nav-text">Lecture 7: Training Neural Networks, part II</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-Fancier-optimization"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 Fancier optimization</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-1-SGD"><span class="nav-number">7.1.1.</span> <span class="nav-text">7.1.1: SGD</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-2-SGD-Momentum"><span class="nav-number">7.1.2.</span> <span class="nav-text">7.1.2: SGD+Momentum</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-3-Nesterov"><span class="nav-number">7.1.3.</span> <span class="nav-text">7.1.3 Nesterov</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-4-AdaGrad"><span class="nav-number">7.1.4.</span> <span class="nav-text">7.1.4 AdaGrad</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-5-RMDProp"><span class="nav-number">7.1.5.</span> <span class="nav-text">7.1.5 RMDProp</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-6-Adam-almost"><span class="nav-number">7.1.6.</span> <span class="nav-text">7.1.6: Adam(almost)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-7-Adam-full-form"><span class="nav-number">7.1.7.</span> <span class="nav-text">7.1.7: Adam(full form)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-1-8-Newton-method"><span class="nav-number">7.1.8.</span> <span class="nav-text">7.1.8 Newton method</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-Model-Ensembles"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 Model Ensembles</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-3-Regulation"><span class="nav-number">7.3.</span> <span class="nav-text">7.3 Regulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-Transfer-Learning"><span class="nav-number">7.4.</span> <span class="nav-text">7.4 Transfer Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-8-Deep-Learning-Software"><span class="nav-number">8.</span> <span class="nav-text">Lecture 8: Deep Learning Software</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Computational-Graphs"><span class="nav-number">8.1.</span> <span class="nav-text">Computational Graphs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#TensorFlow-Neural-Network"><span class="nav-number">8.2.</span> <span class="nav-text">TensorFlow: Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pytorch"><span class="nav-number">8.3.</span> <span class="nav-text">Pytorch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Caffe"><span class="nav-number">8.4.</span> <span class="nav-text">Caffe</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-9-CNN-Architectures"><span class="nav-number">9.</span> <span class="nav-text">Lecture 9: CNN Architectures</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AlexNet"><span class="nav-number">9.1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG"><span class="nav-number">9.2.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">9.3.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet"><span class="nav-number">9.4.</span> <span class="nav-text">ResNet</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-10-RNN"><span class="nav-number">10.</span> <span class="nav-text">Lecture 10: RNN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lecture-11-Applications"><span class="nav-number">11.</span> <span class="nav-text">Lecture 11: Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-1-分割"><span class="nav-number">11.1.</span> <span class="nav-text">11.1 分割</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">芸笙</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.7.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  


  


  

  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .done(function() {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.time + 1);
              })
            
              .fail(function ({ responseJSON }) {
                console.log(`Failed to save Visitor num, with error message: ${responseJSON.error}`);
              })
          } else {
            
              var $element = $(document.getElementById(url));
              $element.find('.leancloud-visitors-count').text('Counter not initialized! More info at console err msg.');
              console.error('ATTENTION! LeanCloud counter has security bug, see how to solve it here: https://github.com/theme-next/hexo-leancloud-counter-security. \n However, you can still use LeanCloud without security, by setting `security` option to `false`.');
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log(`LeanCloud Counter Error: ${responseJSON.code} ${responseJSON.error}`);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + '')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': '',
                'X-LC-Key': '',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            addCount(Counter);
          
        });
    });
  </script>



  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":70,"height":150},"mobile":{"show":true},"log":false});</script></body>
</html>
